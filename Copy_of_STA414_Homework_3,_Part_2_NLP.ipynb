{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of STA414 Homework 3, Part 2: NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MWyKRONCVFYD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junlynli/NLP_Text_Messages/blob/main/Copy_of_STA414_Homework_3%2C_Part_2_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Changelog:\n",
        "\n",
        "Last Update: 2022-03-20\n",
        "\n",
        "Changes:\n",
        "\n",
        "1. token_to_index returns `vocab[token]`\n",
        "2. Improve doscstring for `create_vocab`"
      ],
      "metadata": {
        "id": "ZBQjOo49-ilX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Natural Language Processing\n",
        "\n",
        "### Total Points: 46\n",
        "\n",
        "### Background:\n",
        "In this question you will fit a set of models with the goal of predicting which text messages (SMS) are spam and which are not. \n",
        "\n",
        "You will be experimenting with a variety of different embedding algorithms followed by Logistic Regression."
      ],
      "metadata": {
        "id": "q84Rhsk6FOWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "-_9JVHlrC4sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading, Exploratory Data Analysis and Vocabulary Creation:\n"
      ],
      "metadata": {
        "id": "9mbhj24iFMEJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDe_6kaRCfWs"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from typing import List\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"sms_spam\")\n",
        "texts = [x[\"sms\"] for x in dataset[\"train\"]]\n",
        "labels = [x[\"label\"] for x in dataset[\"train\"]]\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "Z-YrzSxEC2Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_m, y_train, y_m = train_test_split(texts, labels, test_size=0.3, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_m, y_m, test_size = 0.5, random_state=0)"
      ],
      "metadata": {
        "id": "X523_TcvC_4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(x:str) -> List[str]:\n",
        "  \"\"\"\n",
        "  Takes string as an input and returns a list of tokens. This just splits the string x on whitespace\n",
        "  \"\"\"\n",
        "  return x.lower().split(\" \")"
      ],
      "metadata": {
        "id": "4-MHbxEmE2Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[ 5 points ] Q2.a: Determine / Answer the following: \n",
        "\n",
        "*   How many datapoints are in the training set?\n",
        "*   How many total tokens are there in the data?\n",
        "*   What % of training examples are spam?\n",
        "*   What would the accuracy be if for every datapoint we predict it is spam? (explain)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MWyKRONCVFYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of datapoints in the training set \n",
        "print('Number of datapoints in the training set: {}'.format(len(X_train)))\n",
        "# number of total tokens in the data\n",
        "tokens = []\n",
        "for i in texts:\n",
        "  tokens.extend(tokenize(i))\n",
        "print('Number of total tokens in the data: {}'.format(len(tokens)))\n",
        "print('Number of total (distinct) tokens in the data: {}'.format(len(set(tokens))))\n",
        "# % of training examples being spam\n",
        "print('% of training examples being spam: {}'.format(len([y for y in y_train if y == 1])/len(y_train)*100))"
      ],
      "metadata": {
        "id": "FMv85szFDEnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6858d09-9211-4b50-f424-b2901e66fdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of datapoints in the training set: 3901\n",
            "Number of total tokens in the data: 87535\n",
            "Number of total (distinct) tokens in the data: 14856\n",
            "% of training examples being spam: 12.945398615739553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculated above that approximately 13% of the training examples are labeled as spam. Then if we were to predict all the datapoints in the training set as spam, we would only have a 13% accuracy as it is the % of the correctly labeled datapoints. "
      ],
      "metadata": {
        "id": "6RO54TszMH1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[4 points ] Q2.b Vocabulary and Frequencies  "
      ],
      "metadata": {
        "id": "nS6ufPCtGwQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the functions `create_count_dict`, and `create_vocab` below that take as input a dataset (list of untokenized strings - one per datapoint), and an integer representing the desired vocabulary size, and returns the vocabulary as a `dict` whose keys are tokens and whose values are indices."
      ],
      "metadata": {
        "id": "jyKQsSieGzDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_count_dict(dataset:List[str]) -> dict:\n",
        "  \"\"\"\n",
        "  Given a dataset, returns a dictionary of token counts with tokens being keys, and counts being values\n",
        "  \"\"\"\n",
        "  count_dict = {}\n",
        "  # tokenize the texts\n",
        "  tokens = []\n",
        "  for i in dataset:\n",
        "    tokens.extend(tokenize(i))\n",
        "  #store the counts\n",
        "  for token in tokens:\n",
        "    if token not in count_dict.keys():\n",
        "      count_dict[token] = 1\n",
        "    else:\n",
        "      count_dict[token] += 1\n",
        "\n",
        "  return count_dict\n",
        "\n",
        "def create_vocab(dataset:List[str], vocab_size:int) -> dict:\n",
        "  \"\"\"\n",
        "  Given a dataset, returns a dictionary with tokens as keys and indices as values.\n",
        "\n",
        "  {\"most_common_token\": 0, \"second_most_common_token\": 1, ...}\n",
        "  \"\"\"\n",
        "  # 1. Determine the count of all words\n",
        "  count = create_count_dict(dataset)\n",
        "  # 2. Keep the top <vocab_size> tokens\n",
        "  sorted_tokens = sorted(count.items(), key=lambda x: x[1], reverse=True)[:vocab_size]\n",
        "  sorted_tokens = dict(sorted_tokens)\n",
        "  # 3. Create the token to index dict and fill it with just the top tokens\n",
        "  vocab_to_index_dict = {}\n",
        "  index = 0\n",
        "  for token in sorted_tokens.keys():\n",
        "    vocab_to_index_dict[token] = index\n",
        "    index += 1\n",
        "  \n",
        "  return vocab_to_index_dict"
      ],
      "metadata": {
        "id": "ACy4z8-oGSEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper test to at least ensure the constructed vocab is the right size\n",
        "for i in np.random.randint(low = 100, high = 500, size = 5):\n",
        "  assert(len(create_vocab(X_train, vocab_size = i)) == i)"
      ],
      "metadata": {
        "id": "ZWgGXqOtnXaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save your vocab\n",
        "student_created_count_dict = create_count_dict(X_train)\n",
        "student_created_vocab = create_vocab(X_train, vocab_size = 500)"
      ],
      "metadata": {
        "id": "CawjqIMXd3X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[4 points] Q2.c Token Counts"
      ],
      "metadata": {
        "id": "Mx3Q8FkdrDpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.c.a  Is the distribution of token counts below a relatively heavy-tailed distribution?  Would using only the top 30 tokens be sufficient for most applications?"
      ],
      "metadata": {
        "id": "uns15d63rJS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_top_token_distribution(count_dict, num_tokens_to_plot):\n",
        "  \"\"\"\n",
        "  Display a barplot of counts of top <num_tokens_to_plot> tokens in the <count_dict>\n",
        "  \"\"\"\n",
        "  df = pd.DataFrame(data={\"token\": [key for key, _ in count_dict.items()], \"count\": [value for _, value in count_dict.items()]}).sort_values(\"count\", ascending=False)\n",
        "  df[:num_tokens_to_plot].plot(x = \"token\", y = \"count\", kind=\"bar\")"
      ],
      "metadata": {
        "id": "9F3Sc2vtrBCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_top_token_distribution(student_created_count_dict, 10)"
      ],
      "metadata": {
        "id": "FFGiXJuQrfHw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "b49081a0-23f6-4e18-8845-5460901fe923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_top_token_distribution(student_created_count_dict, 30)"
      ],
      "metadata": {
        "id": "Bzx51_xpskwS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "a49532cf-b967-468c-8f73-f8b479bbcc4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_top_token_distribution(student_created_count_dict, 100)"
      ],
      "metadata": {
        "id": "3n6QenP-smAd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "d6e42681-ce71-4421-d557-88d05b92a9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution is a heavy-tailed distribution as it approaches 0 relatively slowly. It looks like using the top 30 samples would suffice. If we compare the plot for the top 30 tokens with the plot for top 100, the general shape is very similar because the distribution is heavy-tailed. So the top 30 samples already captures the distribution nicely. "
      ],
      "metadata": {
        "id": "7wb1aDR0XTSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.c.b [2 points] Take a look at the top 10 tokens in the plot above, do you think the embeddings based on those tokens will be informative for our task?\n"
      ],
      "metadata": {
        "id": "S4woSRLSt6bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top 10 tokens wil not be informative enough for our task. We can observe that the tail only starts to stabilize at around the 20th token. So using the top 10 tokens alone will not provide much information about the distribution."
      ],
      "metadata": {
        "id": "QZCCCcILuG5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.d [ 13 points ] Vocabulary selection and Embeddings"
      ],
      "metadata": {
        "id": "Zx96W-8PtrbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTHING TO DO HERE, JUST SOME CONVENIENCE FUNCTIONS FOR THE FUTURE\n",
        "def index_to_token(index:int, vocabulary:dict) -> str:\n",
        "  \"\"\"\n",
        "  Given an index in the vocabulary, and the vocabulary dict with tokens as keys, and indices as values, returns the token that maps to the given index\n",
        "  \"\"\"\n",
        "  if index > len(vocabulary):\n",
        "    raise ValueError(f\"There is no index {index} in the vocabulary\")\n",
        "  inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
        "  return inv_vocabulary[index]\n",
        "\n",
        "def token_to_index(token:str, vocabulary:dict) -> int:\n",
        "  \"\"\"\n",
        "  Given a token, return the index of that token in the vocabulary\n",
        "  \"\"\"\n",
        "  index = vocabulary.get(token)\n",
        "  if index is None:\n",
        "    raise ValueError(f\"Token: {token} is not in the vocabulary\")\n",
        "  return index"
      ],
      "metadata": {
        "id": "vKCpZXNCH5do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## NOTHING TO DO HERE\n",
        "\n",
        "# BoW with our vocab\n",
        "vectorizer_ours = CountVectorizer(max_features=500, tokenizer=tokenize, vocabulary=student_created_vocab) # Automatic BoW embeddings using your vocab\n",
        "vectorized_data_ours = vectorizer_ours.fit_transform(X_train) # Embedded data\n",
        "\n",
        "# Bow with an auto vocab\n",
        "vectorizer_auto = CountVectorizer(max_features=500) # Automatic BoW embeddings and automatic vocab\n",
        "vectorized_data_auto = vectorizer_auto.fit_transform(X_train) # Embedded data\n",
        "VOCAB = vectorizer_auto.vocabulary_ # For convenience\n",
        "\n",
        "# binary BoW\n",
        "vectorizer_bbow = CountVectorizer(max_features=500, binary=True, vocabulary=VOCAB) # Automatic binary BoW embeddings using the automatically generated vocab above\n",
        "vectorized_data_bbow = vectorizer_bbow.fit_transform(X_train) # Embedded data\n",
        "\n",
        "# TFIDF\n",
        "tfidf_transformer = TfidfTransformer(smooth_idf=False) # TF-IDF transformer for BoW embeddings\n",
        "vectorized_data_tfidf_auto = tfidf_transformer.fit_transform(vectorized_data_auto) # Embedded data"
      ],
      "metadata": {
        "id": "E5GFTnNtJhgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.d.a [ 2 points ] Implement the function `vocabulary_diff` below. It should return a list of tokens that are in `vocab_1` but not in `vocab_2`."
      ],
      "metadata": {
        "id": "mQFvomo9urR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the differences between our vocabulary and the automatically generated one\n",
        "\n",
        "def vocabulary_diff(vocab_1:dict, vocab_2:dict) -> List[str]:\n",
        "  \"\"\"\n",
        "  Given 2 vocabularies, returns all the tokens that are present in vocab_1 but not in vocab_2\n",
        "  \"\"\"\n",
        "  vocab_diff = set(vocab_1.keys()) - set(vocab_2.keys())\n",
        "  return vocab_diff\n",
        "\n",
        "\n",
        "print(vocabulary_diff(student_created_vocab,VOCAB))\n",
        "print(vocabulary_diff(VOCAB, student_created_vocab))"
      ],
      "metadata": {
        "id": "6-CScQ3tKGPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9ba781-3ed6-43eb-d72a-cfdcbf062c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'', 'you,', '1000', '.', \"you're\", 'n', 'a', 'd', 'juz', '&lt;#&gt;', 'lor.\\n', \"i'll\", 'day.\\n', 'wif', 'yeah,', 'day.', 'me\\n', '.\\n', 'sorry,', '?\\n', '5', 'you.\\n', 'u.', '!', 'home.', '1', 'urgent!', 'it.', ',', 'saying', 'you!', \"i'm\", 'i.ll', 'k', 'g', 'stay', 'lor...\\n', '&', \"there's\", 'c', 'me.', \"it's\", '?', '2', '-', 'ok.', 'lor...', 'da.', 'now!', 'u', 'wil', 'later\\n', 'v', 'e', ':)\\n', '6', 'it.\\n', 'tomorrow\\n', 'no.', 'needs', 'i', 'now.', '*', 'it,', '+', 'you.', \"we're\", \"can't\", \"don't\", \"how's\", 'y', 'me.\\n', \"he's\", \"i've\", 'x', \"that's\", '100', '3', \"won't\", '500', 'r', 'you\\n', '4', '&lt;decimal&gt;', '', '\\n', 'b', 'lor.', \"didn't\", '&amp;', '...', 'makes'}\n",
            "{'mob', '10', 'don', 'feeling', 'cs', 'yet', 'hour', 'aight', 'true', 'drive', 'xxx', 'lar', 'urgent', 'min', '18', 'book', 'times', 'www', 'haha', 'baby', 'didn', 'though', 'messages', 'tmr', '150p', 'house', 'dreams', 'girl', 'cool', 'bored', 'comes', 'pa', 'rate', 'liao', 'plz', 'head', 'leh', 'easy', 'com', 'apply', 'okay', 'room', 'month', 'kiss', 'problem', 're', '150ppm', 'orange', 'wife', 'ah', 'anyway', 've', 'office', 'hello', '100', 'll', 'princess', 'amp', 'evening', 'hours', 'co', 'mail', 'bed', 'wk', 'landline', 'fine', '1000', '16', 'uk', '50', 'afternoon', 'code', 'online', 'boy', 'while', 'dad', 'line', 'weekend', 'else', 'worry', 'gt', '000', 'thanx', 'calls', 'haven', 'offer', 'plan', 'sir', 'bus', 'busy', 'word', 'lt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Q2.d.b [3 points] Based on the differences between the two vocabularies, which do you think will represent our datapoints better? Why? "
      ],
      "metadata": {
        "id": "q3gDDBttvyVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'VOCAB' vocabularies represent the datapoints better. The way we tokenize the texts differentiates a word with the same word but at the end of the line. For example, 'you\\n' and 'you!' are different tokens than 'you'. We see that in the 'student_created_vacoab' vocabulary, most of the extra tokens are those that contain symbols, so there is a good chance that the same words are also present in 'VOCAB' but without the symbols. Whereas the extra tokens in 'VOCAB' are legitimate words, hence containing more information about the datapoints. "
      ],
      "metadata": {
        "id": "SSDcv9SYwGJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some convenience constants\n",
        "DOCUMENT_COUNTS = np.bincount(vectorized_data_bbow.indices, minlength=vectorized_data_bbow.shape[1])\n",
        "NUM_DATAPOINTS = len(X_train)\n",
        "VOCAB = vectorizer_auto.vocabulary_"
      ],
      "metadata": {
        "id": "ZDa_SZbdN1tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.d.c [3 points] Implement the `idf` function below which returns the inverse document frequency of a given word as defined by:\n",
        "\n",
        "$$ IDF(w_j) = log(\\frac{N}{N_j}) + 1 $$\n",
        "\n",
        "where:\n",
        "\n",
        "* $N$ is the number of datapoints \n",
        "\n",
        "* $N_j$ is the number of occurences of word $w_j$ in the data"
      ],
      "metadata": {
        "id": "BvRheDH8w7lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def idf(word:str, document_count:np.ndarray=DOCUMENT_COUNTS, num_datapoints:int=NUM_DATAPOINTS) -> float:\n",
        "  \"\"\"\n",
        "  returns the inverse document frequency of a particular word, based on the IDF definition of\n",
        "\n",
        "  IDF(w_j) = log(N / N_j) + 1\n",
        "\n",
        "  \"\"\"\n",
        "  index = VOCAB[word]\n",
        "  return np.log(num_datapoints/document_count[index]) + 1"
      ],
      "metadata": {
        "id": "V-dlIRAPPzSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in np.random.randint(0,500, size=15).tolist():\n",
        "  print(f\"word: {index_to_token(index, VOCAB)}\")\n",
        "  print(f\"your idf: {idf(index_to_token(index, VOCAB))}\")\n",
        "  print(f\"calculated idf: {tfidf_transformer.idf_[index]}\")\n",
        "  assert np.isclose(tfidf_transformer.idf_[index], idf(index_to_token(index, VOCAB)))"
      ],
      "metadata": {
        "id": "7wEYAGnnQi97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed63d242-23fb-425e-b4b4-414df7640ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: head\n",
            "your idf: 6.4357748654504405\n",
            "calculated idf: 6.4357748654504405\n",
            "word: ah\n",
            "your idf: 6.224465771783233\n",
            "calculated idf: 6.224465771783233\n",
            "word: customer\n",
            "your idf: 5.713640148017243\n",
            "calculated idf: 5.713640148017243\n",
            "word: dont\n",
            "your idf: 4.684020730836085\n",
            "calculated idf: 4.684020730836085\n",
            "word: where\n",
            "your idf: 4.862268962242403\n",
            "calculated idf: 4.862268962242403\n",
            "word: it\n",
            "your idf: 3.2216160304603783\n",
            "calculated idf: 3.2216160304603783\n",
            "word: po\n",
            "your idf: 6.0909343791587105\n",
            "calculated idf: 6.0909343791587105\n",
            "word: weekend\n",
            "your idf: 6.177945756148341\n",
            "calculated idf: 6.177945756148341\n",
            "word: co\n",
            "your idf: 5.60542656337701\n",
            "calculated idf: 5.60542656337701\n",
            "word: sms\n",
            "your idf: 5.685469271050547\n",
            "calculated idf: 5.685469271050547\n",
            "word: could\n",
            "your idf: 5.742627684890495\n",
            "calculated idf: 5.742627684890495\n",
            "word: service\n",
            "your idf: 5.58010875539272\n",
            "calculated idf: 5.58010875539272\n",
            "word: forgot\n",
            "your idf: 6.133493993577507\n",
            "calculated idf: 6.133493993577507\n",
            "word: easy\n",
            "your idf: 6.273255935952665\n",
            "calculated idf: 6.273255935952665\n",
            "word: first\n",
            "your idf: 5.555416142802349\n",
            "calculated idf: 5.555416142802349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.d.d [2 points] Implement a binary Bag of Words embedding function that takes in a list of token and a vocab, and returns a numpy array corresponding to the embedding vector"
      ],
      "metadata": {
        "id": "RIYmBJfkxg50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_BoW_embedder(list_of_tokens:List[str], vocab:dict) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Creates a binary Bag of Words embedding of a datapoint represented as a list of tokens, using the vocab\n",
        "  \"\"\"\n",
        "  # Hint: you may want to start with np.zeros()\n",
        "  embedding = np.zeros(len(vocab))\n",
        "  for word in list_of_tokens:\n",
        "    if word in vocab:\n",
        "      index = vocab[word]\n",
        "      embedding[index] = 1\n",
        "\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "AvtjwrUyS0lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test_num in range(15):\n",
        "  sample_token_indices = np.random.randint(0,500, size=10)\n",
        "  input_text = [index_to_token(index, VOCAB) for index in sample_token_indices]\n",
        "  input_text_as_str = \" \".join(input_text)\n",
        "  print(f\"text: {input_text}\")\n",
        "  if test_num == 0:\n",
        "    print(f\"your embedding: {binary_BoW_embedder(input_text, VOCAB)}\")\n",
        "    print(f\"calculated embedding: {vectorizer_bbow.transform([input_text_as_str]).toarray()}\")\n",
        "  assert np.isclose(binary_BoW_embedder(input_text, VOCAB), vectorizer_bbow.transform([input_text_as_str]).toarray()).all()\n",
        "  print(\"Success!\")"
      ],
      "metadata": {
        "id": "j5kCOIlVUMrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ab6454-f3f7-4bb5-d1dc-0f000eb90db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: ['or', 'maybe', 'really', 'yeah', 'time', 'nothing', 'stuff', 'hello', 'wait', 'morning']\n",
            "your embedding: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "calculated embedding: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]]\n",
            "Success!\n",
            "text: ['getting', 'weekly', 'over', 'aight', 'make', 'like', 'receive', 'beautiful', 'wish', 'birthday']\n",
            "Success!\n",
            "text: ['got', 'with', 'he', 'were', 'missed', 'hear', 'wake', 'guess', 'wake', 'bed']\n",
            "Success!\n",
            "text: ['shit', 'best', 'looking', 'per', 'easy', 'worry', 'online', 'enjoy', 'ah', 'plus']\n",
            "Success!\n",
            "text: ['going', 'money', 'sweet', 'code', 'pain', 'about', 'long', 'might', 'remember', 'before']\n",
            "Success!\n",
            "text: ['stuff', 're', 'com', 'show', 'wait', 'watching', 'trying', 'messages', 'txt', 'apply']\n",
            "Success!\n",
            "text: ['how', 'care', 'tomorrow', 'because', 'service', 'sweet', '10', 'talk', 'being', 'dat']\n",
            "Success!\n",
            "text: ['means', 'orange', 'without', 'heart', 'there', 'join', 'babe', 'girl', 'hope', 'wake']\n",
            "Success!\n",
            "text: ['world', 'miss', 'said', 'on', 'missed', 'way', 'wen', 'it', 'mail', 'hear']\n",
            "Success!\n",
            "text: ['number', 'into', 'if', 'line', 'quite', 'show', 'before', 'much', 'though', 'job']\n",
            "Success!\n",
            "text: ['mail', 'he', 'speak', 'year', 'attempt', 'fuck', 'abt', 'oh', 'boy', 'receive']\n",
            "Success!\n",
            "text: ['please', '1st', 'really', 'dunno', 'yeah', 'enough', 'wife', 'bored', 'evening', 'your']\n",
            "Success!\n",
            "text: ['care', 'text', 'give', 'told', 'nice', 'sorry', 'meet', 'xxx', 'left', 'attempt']\n",
            "Success!\n",
            "text: ['amp', 'enjoy', 'home', 'pls', 'mins', 'today', 'anything', 'be', 'this', 'book']\n",
            "Success!\n",
            "text: ['its', 'not', 'cost', 'gt', 'service', 'is', 'wish', 'apply', 'wanna', 'late']\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.d.e [2 points] Implement a Bag of Words embedding function that takes in a list of token and a vocab, and returns a numpy array corresponding to the embedding vector"
      ],
      "metadata": {
        "id": "9QvHSEz2xzAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BoW_embedder(list_of_tokens, vocab):\n",
        "  \"\"\"\n",
        "  Creates a binary Bag of Words embedding of a datapoint represented as a list of tokens, using the vocab\n",
        "  \"\"\"\n",
        "  # Hint: You may want to start with np.zeros()\n",
        "  embedding = np.zeros(len(vocab))\n",
        "  for word in list_of_tokens:\n",
        "    if word in vocab:\n",
        "      index = vocab[word]\n",
        "      embedding[index] = list_of_tokens.count(word)\n",
        "\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "RnWQOaXKaczA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test_num in range(15):\n",
        "  sample_token_indices = np.random.randint(199,204, size=10)\n",
        "  input_text = [index_to_token(index, VOCAB) for index in sample_token_indices]\n",
        "  input_text_as_str = \" \".join(input_text)\n",
        "  print(f\"text: {input_text}\")\n",
        "  if test_num == 0:\n",
        "    print(f\"your embedding: {BoW_embedder(input_text, VOCAB)}\")\n",
        "    print(f\"calculated embedding: {vectorizer_auto.transform([input_text_as_str]).toarray()}\")\n",
        "  assert np.isclose(BoW_embedder(input_text, VOCAB), vectorizer_auto.transform([input_text_as_str]).toarray()).all()\n",
        "  print(\"Success!\")"
      ],
      "metadata": {
        "id": "zH4YTsgghcqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dfa5564-974b-4e3a-ddc0-ad9be4f44d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: ['hours', 'hope', 'holiday', 'hours', 'home', 'hope', 'holiday', 'home', 'home', 'hour']\n",
            "your embedding: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 2. 3. 2. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "calculated embedding: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Success!\n",
            "text: ['hours', 'hope', 'holiday', 'hour', 'hours', 'hour', 'hours', 'hours', 'hope', 'hours']\n",
            "Success!\n",
            "text: ['hour', 'hours', 'hope', 'hope', 'hour', 'home', 'home', 'hours', 'holiday', 'hours']\n",
            "Success!\n",
            "text: ['hour', 'hour', 'hour', 'hour', 'hour', 'hope', 'home', 'hour', 'holiday', 'holiday']\n",
            "Success!\n",
            "text: ['holiday', 'holiday', 'hope', 'holiday', 'hour', 'hours', 'holiday', 'hope', 'hope', 'holiday']\n",
            "Success!\n",
            "text: ['hours', 'holiday', 'hope', 'home', 'hour', 'hope', 'holiday', 'hour', 'holiday', 'holiday']\n",
            "Success!\n",
            "text: ['home', 'hour', 'hour', 'home', 'hope', 'holiday', 'hours', 'holiday', 'holiday', 'hope']\n",
            "Success!\n",
            "text: ['holiday', 'home', 'home', 'hour', 'hours', 'holiday', 'holiday', 'hope', 'home', 'hours']\n",
            "Success!\n",
            "text: ['hour', 'home', 'hour', 'hope', 'hope', 'holiday', 'hours', 'hour', 'home', 'hope']\n",
            "Success!\n",
            "text: ['holiday', 'holiday', 'hour', 'hope', 'hours', 'hope', 'hour', 'hour', 'hope', 'hour']\n",
            "Success!\n",
            "text: ['hope', 'home', 'hope', 'hope', 'hour', 'hour', 'holiday', 'holiday', 'home', 'holiday']\n",
            "Success!\n",
            "text: ['hope', 'hour', 'holiday', 'holiday', 'home', 'home', 'hope', 'hour', 'home', 'holiday']\n",
            "Success!\n",
            "text: ['hour', 'hour', 'holiday', 'home', 'holiday', 'hour', 'hours', 'hours', 'hope', 'holiday']\n",
            "Success!\n",
            "text: ['holiday', 'hope', 'hope', 'hope', 'hour', 'holiday', 'hour', 'hope', 'holiday', 'hour']\n",
            "Success!\n",
            "text: ['hour', 'hope', 'holiday', 'hope', 'holiday', 'hours', 'home', 'home', 'home', 'hope']\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2.e [11 points] Model Fitting"
      ],
      "metadata": {
        "id": "c_2Ff_JaiJ8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "7xkV3-sphmKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_auto = LogisticRegression(random_state=1).fit(vectorized_data_auto, y_train)\n",
        "logreg_bbow = LogisticRegression(random_state=1).fit(vectorized_data_bbow, y_train)\n",
        "logreg_tfidf = LogisticRegression(random_state=1).fit(vectorized_data_tfidf_auto, y_train)"
      ],
      "metadata": {
        "id": "Bkq8Wnm7jSCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.e.a [3 points] Evaluation Metrics\n",
        "Below you can see a set of simple accuracy scores: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score\n",
        "\n",
        "In what situation would accuracy be a good evaluation metric? In what situation it wouldn't? "
      ],
      "metadata": {
        "id": "x166D5jgzcXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" All 1s; accuracy on validation data: {accuracy_score(y_val, np.ones_like(y_val))}\")\n",
        "\n",
        "print(f\" All 0s; accuracy on validation data: {accuracy_score(y_val, np.zeros_like(y_val))}\")\n",
        "\n",
        "print(f\" Logistic Regression on BoW embedding; accuracy on validation data: {accuracy_score(y_val, logreg_auto.predict(vectorizer_auto.transform(X_val)))}\")\n",
        "\n",
        "print(f\" Logistic Regression on binary BoW embedding; accuracy on validation data: {accuracy_score(y_val, logreg_bbow.predict(vectorizer_bbow.transform(X_val)))}\")\n",
        "\n",
        "print(f\" Logistic Regression on TF-IDF embedding; accuracy on validation data: {accuracy_score(y_val, logreg_tfidf.predict(tfidf_transformer.transform(vectorizer_auto.transform(X_val))))}\")\n"
      ],
      "metadata": {
        "id": "rBmtzT6dlmKo",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de002872-db8f-467a-c4ee-69d088e9ace9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All 1s; accuracy on validation data: 0.1255980861244019\n",
            " All 0s; accuracy on validation data: 0.8744019138755981\n",
            " Logistic Regression on BoW embedding; accuracy on validation data: 0.9772727272727273\n",
            " Logistic Regression on binary BoW embedding; accuracy on validation data: 0.9808612440191388\n",
            " Logistic Regression on TF-IDF embedding; accuracy on validation data: 0.972488038277512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The general formula for accuracy is $\\frac{\\text{True positives + True negatives}}{\\text{All Samples}}$. Therefore it would be a good evaluation metric if we are not concerned about the cost of false positives and false negatives (ie. we assign equal cost to false positives and false negatives) and only want to see how many datapoints are correctly labeled. It would not be a good metric, however, if the cost is unequal. Suppose we define positive as being spam, then if we incorrectly classify a job offer as spam (false positive), the cost is way higher than if we were to classify spam as a regular text (false negative). In this case, accuracy score would not be too meaningful. "
      ],
      "metadata": {
        "id": "f3gZZYLy0f37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.e.b [8 points] Answer the following in one-two sentences (2 points each)\n",
        "\n",
        "* What is the interpretation of coefficient $\\beta_j$ with $j > 1$ (so not the intercept) in the Logistic regression model fit on the binary BoW embeddings?\n",
        "\n",
        "* What is the interpretation of coefficient $\\beta_j$ with $j > 1$ (so not the intercept) in the Logistic regression model fit on the BoW embeddings?\n",
        "\n",
        "* If we were to include 1-st order interaction terms in our Logistic Regression model fit on binary BoW embeddings, what would be the interpretation of a coefficient for that interaction ?\n",
        "\n",
        "* Is the above different from fitting a Logistic Regression on a combination of unigrams and bigrams (1-grams, and 2-grams)? Explain."
      ],
      "metadata": {
        "id": "1OWMNrwD0hwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The coefficient represents the effect of one feature (an unique word in the vocabulary) on the fit of the logistic regression.\n",
        "*   The interpretation is same as above. The difference is that in binary BoW, the feature value is either 1 or 0. In BoW, the feature value is the count of that specific word. So the coefficient values can be different because of this difference. \n",
        "*   Apart from the original coefficient that corresponds to one feature, now we will have the interaction of two features. So the additional coefficient for that interaction measures the effect of the combination of the two features. \n",
        "*   This is different from fitting a logistic regression on a combination of unigrams and bigrams. This is because bigrams are defined as a sequence of two **adjacent** elements, so the elements are split in the order of the original tokens. First-order interaction terms can be the interaction of any combinations of two features. For example, suppose we have the text \"abc\". For a combination of unigrams and bigrams, we would have \"a\",\"b\",\"c\",\"ab\",\"bc\". For the addition of first-order interaction, we would have \"a\",\"b\",\"c\",\"ab\",**\"ac\"**,\"bc\".\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JB4UdDyj11C_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.f [4 points] Word Vectors\n",
        "\n",
        "Below we will be using a set of pre-trained word embeddings from the SpaCy library. For the purpose of the questions below, assume that they were trained\n",
        "using a Skip-Gram objective as described in lecture."
      ],
      "metadata": {
        "id": "YFZJTJ9msv_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This installs a full pipeline of tokenizer -> embedder with additional components.\n",
        "\n",
        "import spacy\n",
        "spacy_pipeline = spacy.load('en_core_web_md') # If this line produces an error,\n",
        "# you might need to re-start the runtime after running the first cell."
      ],
      "metadata": {
        "id": "9vllrC6GmURD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.f.a [2 points] Implement the function get_document_embedding_from_spacy below to return the document embedding as the average of all vector embeddings.\n",
        "\n",
        "You are not allowed to use `tokens.vector` in your functions, but you can check against it that your solution is accurate"
      ],
      "metadata": {
        "id": "MHMjDoj22TiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_document_embedding_from_spacy(text:str) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  returns the embedding vector representing the entire text by averaging the \n",
        "  word level embeddings.\n",
        "\n",
        "  !! DO NOT USE tokens.vector directly!! \n",
        "  \"\"\"\n",
        "  tokens = spacy_pipeline(text)\n",
        "\n",
        "  # Hint: you can iterate over each token in `tokens` and access the underlying vector with .vector\n",
        "  vectors = []\n",
        "  for token in tokens:\n",
        "    vectors.append(token.vector)\n",
        "  document_embedding = np.mean(vectors, axis=0)\n",
        "\n",
        "  return document_embedding"
      ],
      "metadata": {
        "id": "Lt-s7xVZm_5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST FOR THE get_document_embedding_from_spacy function\n",
        "for test_num in range(15):\n",
        "  sample_token_indices = np.random.randint(0,500, size=10)\n",
        "  input_text = [index_to_token(index, VOCAB) for index in sample_token_indices]\n",
        "  input_text_as_str = \" \".join(input_text)\n",
        "  print(f\"text: {input_text}\")\n",
        "  if test_num == 0:\n",
        "    print(f\"your embedding: {get_document_embedding_from_spacy(input_text_as_str)}\")\n",
        "    print(f\"calculated embedding: {spacy_pipeline(input_text_as_str).vector}\")\n",
        "  assert np.isclose(get_document_embedding_from_spacy(input_text_as_str), spacy_pipeline(input_text_as_str).vector).all()\n",
        "  print(\"Success!\")"
      ],
      "metadata": {
        "id": "AWf-PZXlp2Do",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ac3fa4-171d-4174-e72f-a1c3cebb0426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: ['if', 'waiting', 'good', 'world', 'getting', 'bus', 'nice', 'thats', 'morning', 'on']\n",
            "your embedding: [ 5.77830411e-02  1.94456354e-01 -2.19950899e-01 -3.78643535e-02\n",
            "  9.56407264e-02  4.53359969e-02  8.85836128e-03 -1.45602450e-01\n",
            " -4.53000925e-02  2.39745450e+00 -2.43452176e-01 -1.15168363e-01\n",
            "  7.67919049e-02 -3.75229083e-02 -3.66774261e-01 -2.93011479e-02\n",
            " -1.47375271e-01  1.11763370e+00 -1.02154732e-01 -1.30321989e-02\n",
            " -3.26862708e-02  1.12621538e-01 -8.88800062e-03 -4.74057309e-02\n",
            "  5.76359145e-02 -2.09286213e-02 -2.14628633e-02 -1.40165001e-01\n",
            "  1.13103099e-01 -4.19531800e-02 -1.00099802e-01 -1.27693549e-01\n",
            " -6.14572596e-03  2.54744720e-02  8.97536427e-02  3.00560016e-02\n",
            "  7.57170990e-02  6.76214546e-02 -1.65086001e-01 -7.52437711e-02\n",
            "  1.71010010e-02 -2.83627287e-02 -8.08163583e-02  5.34527265e-02\n",
            "  2.79981866e-02  1.70090809e-01 -1.20148748e-01 -4.45693620e-02\n",
            " -1.18348815e-01 -6.59102127e-02 -1.07713267e-01  7.12039992e-02\n",
            "  1.02204541e-02  1.63156196e-01  3.67557406e-02  4.25188988e-02\n",
            " -7.49495476e-02 -7.51049966e-02 -3.92942689e-02 -7.31116310e-02\n",
            " -7.58016407e-02 -1.53665036e-01 -7.58426413e-02  1.45814538e-01\n",
            "  1.39052555e-01 -1.99691840e-02  3.19267809e-02  2.16017440e-01\n",
            "  1.60258133e-02  2.11663008e-01  1.29430652e-01  1.45465434e-02\n",
            "  2.75215894e-01 -3.38761769e-02  2.39774603e-02  1.34591851e-02\n",
            "  8.93220603e-02 -4.04253602e-02 -1.21790081e-01  2.81314641e-01\n",
            "  1.71319470e-01  9.96569917e-02 -6.07244521e-02  7.92801008e-03\n",
            "  7.88530931e-02 -5.62324524e-02  2.09131196e-01 -1.77664891e-01\n",
            "  1.87249988e-01  5.21639027e-02 -1.16070040e-01  1.21415980e-01\n",
            " -1.88078448e-01 -1.19548365e-01  9.60018113e-02 -4.38139103e-02\n",
            "  2.32090964e-03 -1.32953092e-01  8.41478705e-02  1.85054868e-01\n",
            " -8.41945037e-03 -2.99953651e-02  7.55881984e-03 -9.90392864e-02\n",
            "  7.50173628e-02 -5.58121324e-01  1.22538276e-01 -1.34616550e-02\n",
            "  3.80059145e-02  4.47868183e-02  4.05188203e-02 -2.27690279e-01\n",
            "  1.86875820e-01  5.14365472e-02  1.93801820e-01 -3.64411809e-02\n",
            " -1.44557301e-02 -5.25446385e-02  2.81055402e-02  1.81131884e-02\n",
            "  1.36125743e-01 -2.71609947e-02  1.64977228e-03 -8.94663334e-02\n",
            " -7.60962889e-02  9.75233540e-02 -8.70600343e-03 -1.89308658e-01\n",
            " -2.01377291e-02 -1.95053622e-01  8.55110958e-02 -6.78581819e-02\n",
            "  6.33580908e-02 -1.04210012e-01  8.33785534e-03 -8.84336513e-03\n",
            "  7.29555786e-02  3.47474553e-02 -4.32326384e-02 -3.76250595e-02\n",
            " -1.72876632e+00 -3.99908703e-03  2.39697650e-01 -9.18780044e-02\n",
            " -2.08243653e-02 -1.48534998e-01 -1.85276210e-01 -3.39964591e-02\n",
            "  3.64831835e-02 -5.29532693e-02 -1.70573100e-01  1.33184642e-01\n",
            "  5.06642908e-02 -1.11329272e-01 -5.35878167e-02 -1.18754603e-01\n",
            "  2.14977264e-02  7.34829009e-02  6.99280053e-02 -1.67445555e-01\n",
            " -2.42386729e-01  6.83415532e-02  2.06939504e-02 -6.72362046e-04\n",
            "  1.20769568e-01 -1.90180823e-01  2.52816360e-02 -5.28243557e-02\n",
            "  2.73993820e-01  3.08701806e-02  6.67453604e-03 -3.34038176e-02\n",
            " -1.81572617e-03 -1.70713007e-01 -1.31287863e-02  9.71101001e-02\n",
            " -7.00697303e-02  1.47352424e-02 -9.59532708e-02  4.13172506e-02\n",
            " -4.61540855e-02 -1.29487813e-01 -1.83645815e-01 -6.30893409e-02\n",
            " -7.71036446e-02  7.90366810e-03  5.18748956e-03  8.91946331e-02\n",
            "  3.44340913e-02 -3.21366377e-02 -3.08750924e-02  4.77984548e-02\n",
            " -1.63953856e-01  3.89653980e-03 -4.24803644e-02  9.74003598e-02\n",
            "  5.23957275e-02 -8.01101923e-02 -8.79954547e-02  6.12446330e-02\n",
            " -3.98226343e-02 -3.16522792e-02 -7.91558996e-02 -1.51726389e-02\n",
            "  2.19831899e-01 -1.16004497e-02 -2.59728972e-02 -6.55533299e-02\n",
            "  7.10451603e-02  1.31500848e-02 -9.51937363e-02 -1.72458276e-01\n",
            " -5.34484498e-02 -1.76113456e-01 -6.88379258e-03  1.34982362e-01\n",
            " -1.33643910e-01 -1.31464005e-01 -1.90599099e-01  3.11936382e-02\n",
            " -9.91306379e-02 -8.06855857e-02 -7.12927282e-02  1.50225461e-01\n",
            "  4.54660021e-02 -6.76870942e-02  6.99320715e-03  1.03146367e-01\n",
            " -2.00237855e-02  1.05326630e-01  2.77101826e-02  4.09435891e-02\n",
            "  6.43917769e-02  8.79969969e-02  5.44019565e-02 -1.15093820e-01\n",
            " -3.51989083e-02 -1.92908093e-01 -1.69472903e-01  2.49583095e-01\n",
            "  1.08928911e-01  1.09352827e-01 -6.13416098e-02  1.68795913e-01\n",
            " -1.93802733e-02 -1.37902275e-01 -1.17174223e-01 -9.32242796e-02\n",
            " -9.72017497e-02  5.91333658e-02 -1.08551003e-01 -1.68897539e-01\n",
            " -1.18406983e-02  3.81493033e-03  2.55680457e-02  3.11935812e-01\n",
            "  5.55644147e-02 -1.61560282e-01 -4.84270900e-02  4.29910049e-02\n",
            "  1.38611004e-01  1.44780919e-01  2.61982325e-02  1.22238010e-01\n",
            "  1.61081806e-01 -8.24994519e-02 -2.62260921e-02  1.13906451e-01\n",
            "  2.47419029e-01  8.57679397e-02 -1.04208142e-02  1.13211805e-02\n",
            " -1.67034730e-01 -2.26016000e-01 -1.49370879e-02  1.76304951e-01\n",
            " -1.41990290e-03  2.01474018e-02  1.89876169e-01  3.07975382e-01\n",
            "  7.46171772e-02 -1.47969633e-01 -3.28390263e-02  8.42370465e-02\n",
            "  1.21962661e-02 -5.07738143e-02  1.59365445e-01 -7.19816387e-02\n",
            " -5.22093661e-02  8.15249979e-02 -1.55073911e-01 -1.05681811e-02\n",
            "  8.00332725e-02 -1.61067352e-01  1.61134288e-01 -4.52299155e-02\n",
            "  6.08492754e-02 -1.67997643e-01 -2.93040462e-02  1.32381380e-01]\n",
            "calculated embedding: [ 5.77830411e-02  1.94456354e-01 -2.19950899e-01 -3.78643535e-02\n",
            "  9.56407264e-02  4.53359969e-02  8.85836128e-03 -1.45602450e-01\n",
            " -4.53000925e-02  2.39745450e+00 -2.43452176e-01 -1.15168363e-01\n",
            "  7.67919049e-02 -3.75229083e-02 -3.66774261e-01 -2.93011479e-02\n",
            " -1.47375271e-01  1.11763370e+00 -1.02154732e-01 -1.30321989e-02\n",
            " -3.26862708e-02  1.12621538e-01 -8.88800062e-03 -4.74057309e-02\n",
            "  5.76359145e-02 -2.09286213e-02 -2.14628633e-02 -1.40165001e-01\n",
            "  1.13103099e-01 -4.19531800e-02 -1.00099802e-01 -1.27693549e-01\n",
            " -6.14572596e-03  2.54744720e-02  8.97536427e-02  3.00560016e-02\n",
            "  7.57170990e-02  6.76214546e-02 -1.65086001e-01 -7.52437711e-02\n",
            "  1.71010010e-02 -2.83627287e-02 -8.08163583e-02  5.34527265e-02\n",
            "  2.79981866e-02  1.70090809e-01 -1.20148748e-01 -4.45693620e-02\n",
            " -1.18348815e-01 -6.59102127e-02 -1.07713267e-01  7.12039992e-02\n",
            "  1.02204541e-02  1.63156196e-01  3.67557406e-02  4.25188988e-02\n",
            " -7.49495476e-02 -7.51049966e-02 -3.92942689e-02 -7.31116310e-02\n",
            " -7.58016407e-02 -1.53665036e-01 -7.58426413e-02  1.45814538e-01\n",
            "  1.39052555e-01 -1.99691840e-02  3.19267809e-02  2.16017440e-01\n",
            "  1.60258133e-02  2.11663008e-01  1.29430652e-01  1.45465434e-02\n",
            "  2.75215894e-01 -3.38761769e-02  2.39774603e-02  1.34591851e-02\n",
            "  8.93220603e-02 -4.04253602e-02 -1.21790081e-01  2.81314641e-01\n",
            "  1.71319470e-01  9.96569917e-02 -6.07244521e-02  7.92801008e-03\n",
            "  7.88530931e-02 -5.62324524e-02  2.09131196e-01 -1.77664891e-01\n",
            "  1.87249988e-01  5.21639027e-02 -1.16070040e-01  1.21415980e-01\n",
            " -1.88078448e-01 -1.19548365e-01  9.60018113e-02 -4.38139103e-02\n",
            "  2.32090964e-03 -1.32953092e-01  8.41478705e-02  1.85054868e-01\n",
            " -8.41945037e-03 -2.99953651e-02  7.55881984e-03 -9.90392864e-02\n",
            "  7.50173628e-02 -5.58121324e-01  1.22538276e-01 -1.34616550e-02\n",
            "  3.80059145e-02  4.47868183e-02  4.05188203e-02 -2.27690279e-01\n",
            "  1.86875820e-01  5.14365472e-02  1.93801820e-01 -3.64411809e-02\n",
            " -1.44557301e-02 -5.25446385e-02  2.81055402e-02  1.81131884e-02\n",
            "  1.36125743e-01 -2.71609947e-02  1.64977228e-03 -8.94663334e-02\n",
            " -7.60962889e-02  9.75233540e-02 -8.70600343e-03 -1.89308658e-01\n",
            " -2.01377291e-02 -1.95053622e-01  8.55110958e-02 -6.78581819e-02\n",
            "  6.33580908e-02 -1.04210012e-01  8.33785534e-03 -8.84336513e-03\n",
            "  7.29555786e-02  3.47474553e-02 -4.32326384e-02 -3.76250595e-02\n",
            " -1.72876632e+00 -3.99908703e-03  2.39697650e-01 -9.18780044e-02\n",
            " -2.08243653e-02 -1.48534998e-01 -1.85276210e-01 -3.39964591e-02\n",
            "  3.64831835e-02 -5.29532693e-02 -1.70573100e-01  1.33184642e-01\n",
            "  5.06642908e-02 -1.11329272e-01 -5.35878167e-02 -1.18754603e-01\n",
            "  2.14977264e-02  7.34829009e-02  6.99280053e-02 -1.67445555e-01\n",
            " -2.42386729e-01  6.83415532e-02  2.06939504e-02 -6.72362046e-04\n",
            "  1.20769568e-01 -1.90180823e-01  2.52816360e-02 -5.28243557e-02\n",
            "  2.73993820e-01  3.08701806e-02  6.67453604e-03 -3.34038176e-02\n",
            " -1.81572617e-03 -1.70713007e-01 -1.31287863e-02  9.71101001e-02\n",
            " -7.00697303e-02  1.47352424e-02 -9.59532708e-02  4.13172506e-02\n",
            " -4.61540855e-02 -1.29487813e-01 -1.83645815e-01 -6.30893409e-02\n",
            " -7.71036446e-02  7.90366810e-03  5.18748956e-03  8.91946331e-02\n",
            "  3.44340913e-02 -3.21366377e-02 -3.08750924e-02  4.77984548e-02\n",
            " -1.63953856e-01  3.89653980e-03 -4.24803644e-02  9.74003598e-02\n",
            "  5.23957275e-02 -8.01101923e-02 -8.79954547e-02  6.12446330e-02\n",
            " -3.98226343e-02 -3.16522792e-02 -7.91558996e-02 -1.51726389e-02\n",
            "  2.19831899e-01 -1.16004497e-02 -2.59728972e-02 -6.55533299e-02\n",
            "  7.10451603e-02  1.31500848e-02 -9.51937363e-02 -1.72458276e-01\n",
            " -5.34484498e-02 -1.76113456e-01 -6.88379258e-03  1.34982362e-01\n",
            " -1.33643910e-01 -1.31464005e-01 -1.90599099e-01  3.11936382e-02\n",
            " -9.91306379e-02 -8.06855857e-02 -7.12927282e-02  1.50225461e-01\n",
            "  4.54660021e-02 -6.76870942e-02  6.99320715e-03  1.03146367e-01\n",
            " -2.00237855e-02  1.05326630e-01  2.77101826e-02  4.09435891e-02\n",
            "  6.43917769e-02  8.79969969e-02  5.44019565e-02 -1.15093820e-01\n",
            " -3.51989083e-02 -1.92908093e-01 -1.69472903e-01  2.49583095e-01\n",
            "  1.08928911e-01  1.09352827e-01 -6.13416098e-02  1.68795913e-01\n",
            " -1.93802733e-02 -1.37902275e-01 -1.17174223e-01 -9.32242796e-02\n",
            " -9.72017497e-02  5.91333658e-02 -1.08551003e-01 -1.68897539e-01\n",
            " -1.18406983e-02  3.81493033e-03  2.55680457e-02  3.11935812e-01\n",
            "  5.55644147e-02 -1.61560282e-01 -4.84270900e-02  4.29910049e-02\n",
            "  1.38611004e-01  1.44780919e-01  2.61982325e-02  1.22238010e-01\n",
            "  1.61081806e-01 -8.24994519e-02 -2.62260921e-02  1.13906451e-01\n",
            "  2.47419029e-01  8.57679397e-02 -1.04208142e-02  1.13211805e-02\n",
            " -1.67034730e-01 -2.26016000e-01 -1.49370879e-02  1.76304951e-01\n",
            " -1.41990290e-03  2.01474018e-02  1.89876169e-01  3.07975382e-01\n",
            "  7.46171772e-02 -1.47969633e-01 -3.28390263e-02  8.42370465e-02\n",
            "  1.21962661e-02 -5.07738143e-02  1.59365445e-01 -7.19816387e-02\n",
            " -5.22093661e-02  8.15249979e-02 -1.55073911e-01 -1.05681811e-02\n",
            "  8.00332725e-02 -1.61067352e-01  1.61134288e-01 -4.52299155e-02\n",
            "  6.08492754e-02 -1.67997643e-01 -2.93040462e-02  1.32381380e-01]\n",
            "Success!\n",
            "text: ['thing', 'still', 'text', 'enjoy', 'here', 'then', 'dun', 'anything', 'guy', 'weekly']\n",
            "Success!\n",
            "text: ['vouchers', 'sleep', 'who', 'some', 'aight', 'little', 'full', 'as', 'leave', 'shopping']\n",
            "Success!\n",
            "text: ['bed', 'use', 'messages', 'my', 'am', 'last', 'morning', 'never', 'cool', 'name']\n",
            "Success!\n",
            "text: ['show', 'wan', 'word', 'wat', 'part', 'on', 'find', '1000', 'apply', 'hello']\n",
            "Success!\n",
            "text: ['beautiful', 'actually', 'gt', 'just', 'full', 'without', 'back', 'sms', '150ppm', 'cost']\n",
            "Success!\n",
            "text: ['speak', 'let', 'around', 'much', 'room', 'yo', 'ok', 'half', 'coming', 'cost']\n",
            "Success!\n",
            "text: ['he', 'word', 'dis', 'heart', 'make', 'while', 'tmr', 'why', 'getting', 'many']\n",
            "Success!\n",
            "text: ['pick', 'dun', 'after', 'music', 'xxx', 'know', 'being', 'big', 'pick', 'service']\n",
            "Success!\n",
            "text: ['gud', 'much', 'didnt', 'next', 'txt', 'ready', 'for', 'want', 'driving', 'friends']\n",
            "Success!\n",
            "text: ['xxx', 'stuff', 'win', 'mobile', 'da', 'to', 'than', 'offer', 'lot', 'guy']\n",
            "Success!\n",
            "text: ['ask', 'without', 'got', 'love', 'always', 'school', 'one', 'those', 'thk', 'able']\n",
            "Success!\n",
            "text: ['pain', 'tell', 'out', 'are', 'guaranteed', 'again', 'person', 'messages', 'years', 'them']\n",
            "Success!\n",
            "text: ['everything', 'weekly', 'sleep', 'why', 'face', 'com', 'say', 'phone', 'next', 'it']\n",
            "Success!\n",
            "text: ['walk', 'ready', 'bed', 'free', 'haven', 'bored', 'hear', 'dreams', 'first', 'over']\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This might take around 1 minute to run\n",
        "X_train_spacy = np.vstack([get_document_embedding_from_spacy(x) for x in X_train])\n",
        "X_valid_spacy = np.vstack([get_document_embedding_from_spacy(x) for x in X_val])\n",
        "logreg_w2v = LogisticRegression(random_state=1, max_iter=1000).fit(X_train_spacy, y_train)\n",
        "print(f\" Logistic Regression on Word2Vec embedding; accuracy on validation data: {accuracy_score(y_val , logreg_w2v.predict(X_valid_spacy))}\")"
      ],
      "metadata": {
        "id": "wsTeOHpY2g0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c5e727-97b4-4c03-b45e-aa04ac06c3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Logistic Regression on Word2Vec embedding; accuracy on validation data: 0.9677033492822966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.f.b [2 points] Implement the function get_document_embedding_from_spacy_alt below to return the document embedding based on the token vectors.\n",
        "\n",
        "Feel free to let your imagination run, or go with simplicity. Don't use the average as above. Feel free to play around and see what works and what doesn't."
      ],
      "metadata": {
        "id": "CUgPJT303570"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_document_embedding_from_spacy_alt(text:str) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  returns the embedding vector representing the entire text by whatever means you\n",
        "  would like based on the token level embeddings\n",
        "  \"\"\"\n",
        "  tokens = spacy_pipeline(text)\n",
        "  # filters out symbols, numbers, and others\n",
        "  filter = ['SYM', 'NUM', 'X']\n",
        "\n",
        "  vectors = []\n",
        "  for token in tokens:\n",
        "    if token.pos_ not in filter:\n",
        "      vectors.append(token.vector)\n",
        "  \n",
        "  document_embedding = np.max(vectors, axis=0)\n",
        "\n",
        "  return document_embedding"
      ],
      "metadata": {
        "id": "4PK2CpdcuOil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This might take around a 1 minute\n",
        "X_train_spacy_alt = np.vstack([get_document_embedding_from_spacy_alt(x) for x in X_train])\n",
        "X_valid_spacy_alt = np.vstack([get_document_embedding_from_spacy_alt(x) for x in X_val])\n",
        "logreg_w2v_alt = LogisticRegression(random_state=1, max_iter=1000).fit(X_train_spacy_alt, y_train)\n",
        "print(f\" Logistic Regression on Word2Vec alternative embedding; accuracy on validation data: {accuracy_score(y_val , logreg_w2v_alt.predict(X_valid_spacy_alt))}\")"
      ],
      "metadata": {
        "id": "UrgKFCLjp2ko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5525b5-e5b5-4f23-f0a3-1c09f18291ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Logistic Regression on Word2Vec alternative embedding; accuracy on validation data: 0.9748803827751196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining Embeddings [Free]"
      ],
      "metadata": {
        "id": "IzxBMiHBvaum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_combined = np.hstack((X_train_spacy, vectorized_data_auto.toarray()))\n",
        "X_val_combined = np.hstack((X_valid_spacy, vectorizer_auto.transform(X_val).toarray()))\n",
        "logreg_combined = LogisticRegression(random_state=1, max_iter=1000).fit(X_train_combined, y_train)\n",
        "print(f\" Logistic Regression on combined W2V and BoW embeddings; accuracy on validation data: {logreg_combined.score(X_val_combined, y_val)}\")"
      ],
      "metadata": {
        "id": "eMx3NlAquqMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d546bdd6-779c-40c9-d082-81fe03f02d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Logistic Regression on combined W2V and BoW embeddings; accuracy on validation data: 0.9832535885167464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[2 points] Q2.g Transformer Model\n",
        "\n",
        "Below we will be using embeddings produced by a \"Transformer\" model (we will learn more about these in week 12/13), for the embedding of each token, it aims to incorporate not only the meaning of the token, but also the specific context in which it occurs here, with the context being a fixed window of size 512."
      ],
      "metadata": {
        "id": "kN4fRAjfxioQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert-tiny-finetuned-sms-spam-detection\") # This model has been trained on this data\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/bert-tiny-finetuned-sms-spam-detection\", output_hidden_states = True)"
      ],
      "metadata": {
        "id": "fWudJJ2Qxhxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.g.a [2 points] Finish the implementation of get_embedding_from_transformer to produce a document level embedding given some text. "
      ],
      "metadata": {
        "id": "1F6WNlHq57yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_from_transformer(text:str) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Returns an embedding from a transformer model produced by averaging the token level embeddings\n",
        "  \"\"\"\n",
        "  tokenized = tokenizer([text])\n",
        "  for key, value in tokenized.items():\n",
        "    tokenized[key] = torch.LongTensor(value)\n",
        "  with torch.no_grad():\n",
        "    token_embeddings = model.forward(**tokenized)[\"hidden_states\"][0].numpy()[0]\n",
        "  # print(token_embeddings.shape)\n",
        "  document_embedding = np.mean(token_embeddings, axis=0)\n",
        "  return document_embedding"
      ],
      "metadata": {
        "id": "u49GVcFtxsV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JUST SOME TESTING THAT IT RUNS AND PRODUCES THE RIGHT SIZE\n",
        "for e in [get_embedding_from_transformer(x) for x in X_train[:3]]:\n",
        "  assert e.shape == (128,)"
      ],
      "metadata": {
        "id": "e2zZOpJb6ZNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This might take around a 1 minute\n",
        "X_train_transformer = np.vstack([get_embedding_from_transformer(x) for x in X_train])\n",
        "X_valid_transformer = np.vstack([get_embedding_from_transformer(x) for x in X_val])\n",
        "logreg_transformer = LogisticRegression(random_state=1, max_iter=1000).fit(X_train_transformer, y_train)\n",
        "print(f\" Logistic Regression on Transformer embedding accuracy on validation: {logreg_transformer.score(X_valid_transformer, y_val)}\")"
      ],
      "metadata": {
        "id": "RQ_EvOXAylxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94173b17-3666-4cec-ee20-43f0f45cb9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Logistic Regression on Transformer embedding accuracy on validation: 0.9844497607655502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.h [3 points] Which spam detection method worked best? Why do you think that is? What are some considerations you would need to make to use this model in practice? "
      ],
      "metadata": {
        "id": "RLt2Q4iWfeP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer model worked best. This is because contextual information is important in language models, and words can refer to different things in different contexts. Some considerations are how the text is split when feeding it to the model. A sentence can have drastically different meanings depending on the way that it is split, which can affect the context that this method depends on. "
      ],
      "metadata": {
        "id": "P5LEwqxPpz_P"
      }
    }
  ]
}